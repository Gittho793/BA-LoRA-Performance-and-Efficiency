{
  "mmlu": {
    "command": "lm_eval --model hf --model_args pretrained=unsloth/Meta-Llama-3.1-8B-Instruct,peft=../../../unslothLora/Llama/pdf-ds-new-llama-3.1-8b-v0.3-4bit-lora-r8-a16,load_in_4bit=True --tasks mmlu --num_fewshot 4 --batch_size auto --device cuda --output_path ../../results/mmlu/pdf-ds-new-llama-3.1-8b-v0.3-4bit-lora-r8-a16/mmlu_pdf-ds-new-llama-3.1-8b-v0.3-4bit-lora-r8-a16.json",
    "stdout": "INFO 08-23 09:57:37 [__init__.py:244] Automatically detected platform cuda.\nPassed argument batch_size = auto:1. Detecting largest batch size\nDetermined largest batch size: 32\nhf (pretrained=unsloth/Meta-Llama-3.1-8B-Instruct,peft=../../../unslothLora/Llama/pdf-ds-new-llama-3.1-8b-v0.3-4bit-lora-r8-a16,load_in_4bit=True), gen_kwargs: (None), limit: None, num_fewshot: 4, batch_size: auto (32)\n|                 Tasks                 |Version|Filter|n-shot|Metric|   |Value |   |Stderr|\n|---------------------------------------|------:|------|-----:|------|---|-----:|---|-----:|\n|mmlu                                   |      2|none  |      |acc   |\u2191  |0.6350|\u00b1  |0.0039|\n| - humanities                          |      2|none  |      |acc   |\u2191  |0.5881|\u00b1  |0.0068|\n|  - formal_logic                       |      1|none  |     4|acc   |\u2191  |0.4365|\u00b1  |0.0444|\n|  - high_school_european_history       |      1|none  |     4|acc   |\u2191  |0.7394|\u00b1  |0.0343|\n|  - high_school_us_history             |      1|none  |     4|acc   |\u2191  |0.8529|\u00b1  |0.0249|\n|  - high_school_world_history          |      1|none  |     4|acc   |\u2191  |0.7848|\u00b1  |0.0268|\n|  - international_law                  |      1|none  |     4|acc   |\u2191  |0.8182|\u00b1  |0.0352|\n|  - jurisprudence                      |      1|none  |     4|acc   |\u2191  |0.7407|\u00b1  |0.0424|\n|  - logical_fallacies                  |      1|none  |     4|acc   |\u2191  |0.7546|\u00b1  |0.0338|\n|  - moral_disputes                     |      1|none  |     4|acc   |\u2191  |0.7081|\u00b1  |0.0245|\n|  - moral_scenarios                    |      1|none  |     4|acc   |\u2191  |0.4503|\u00b1  |0.0166|\n|  - philosophy                         |      1|none  |     4|acc   |\u2191  |0.6785|\u00b1  |0.0265|\n|  - prehistory                         |      1|none  |     4|acc   |\u2191  |0.7037|\u00b1  |0.0254|\n|  - professional_law                   |      1|none  |     4|acc   |\u2191  |0.4570|\u00b1  |0.0127|\n|  - world_religions                    |      1|none  |     4|acc   |\u2191  |0.8187|\u00b1  |0.0295|\n| - other                               |      2|none  |      |acc   |\u2191  |0.6962|\u00b1  |0.0080|\n|  - business_ethics                    |      1|none  |     4|acc   |\u2191  |0.7300|\u00b1  |0.0446|\n|  - clinical_knowledge                 |      1|none  |     4|acc   |\u2191  |0.6830|\u00b1  |0.0286|\n|  - college_medicine                   |      1|none  |     4|acc   |\u2191  |0.6358|\u00b1  |0.0367|\n|  - global_facts                       |      1|none  |     4|acc   |\u2191  |0.3500|\u00b1  |0.0479|\n|  - human_aging                        |      1|none  |     4|acc   |\u2191  |0.6547|\u00b1  |0.0319|\n|  - management                         |      1|none  |     4|acc   |\u2191  |0.7961|\u00b1  |0.0399|\n|  - marketing                          |      1|none  |     4|acc   |\u2191  |0.8504|\u00b1  |0.0234|\n|  - medical_genetics                   |      1|none  |     4|acc   |\u2191  |0.7700|\u00b1  |0.0423|\n|  - miscellaneous                      |      1|none  |     4|acc   |\u2191  |0.8020|\u00b1  |0.0142|\n|  - nutrition                          |      1|none  |     4|acc   |\u2191  |0.7353|\u00b1  |0.0253|\n|  - professional_accounting            |      1|none  |     4|acc   |\u2191  |0.4823|\u00b1  |0.0298|\n|  - professional_medicine              |      1|none  |     4|acc   |\u2191  |0.6765|\u00b1  |0.0284|\n|  - virology                           |      1|none  |     4|acc   |\u2191  |0.5241|\u00b1  |0.0389|\n| - social sciences                     |      2|none  |      |acc   |\u2191  |0.7371|\u00b1  |0.0077|\n|  - econometrics                       |      1|none  |     4|acc   |\u2191  |0.4298|\u00b1  |0.0466|\n|  - high_school_geography              |      1|none  |     4|acc   |\u2191  |0.8131|\u00b1  |0.0278|\n|  - high_school_government_and_politics|      1|none  |     4|acc   |\u2191  |0.8756|\u00b1  |0.0238|\n|  - high_school_macroeconomics         |      1|none  |     4|acc   |\u2191  |0.6667|\u00b1  |0.0239|\n|  - high_school_microeconomics         |      1|none  |     4|acc   |\u2191  |0.7353|\u00b1  |0.0287|\n|  - high_school_psychology             |      1|none  |     4|acc   |\u2191  |0.8330|\u00b1  |0.0160|\n|  - human_sexuality                    |      1|none  |     4|acc   |\u2191  |0.7481|\u00b1  |0.0381|\n|  - professional_psychology            |      1|none  |     4|acc   |\u2191  |0.6389|\u00b1  |0.0194|\n|  - public_relations                   |      1|none  |     4|acc   |\u2191  |0.6364|\u00b1  |0.0461|\n|  - security_studies                   |      1|none  |     4|acc   |\u2191  |0.7429|\u00b1  |0.0280|\n|  - sociology                          |      1|none  |     4|acc   |\u2191  |0.8507|\u00b1  |0.0252|\n|  - us_foreign_policy                  |      1|none  |     4|acc   |\u2191  |0.8800|\u00b1  |0.0327|\n| - stem                                |      2|none  |      |acc   |\u2191  |0.5449|\u00b1  |0.0086|\n|  - abstract_algebra                   |      1|none  |     4|acc   |\u2191  |0.2800|\u00b1  |0.0451|\n|  - anatomy                            |      1|none  |     4|acc   |\u2191  |0.6222|\u00b1  |0.0419|\n|  - astronomy                          |      1|none  |     4|acc   |\u2191  |0.7039|\u00b1  |0.0372|\n|  - college_biology                    |      1|none  |     4|acc   |\u2191  |0.7361|\u00b1  |0.0369|\n|  - college_chemistry                  |      1|none  |     4|acc   |\u2191  |0.4500|\u00b1  |0.0500|\n|  - college_computer_science           |      1|none  |     4|acc   |\u2191  |0.5200|\u00b1  |0.0502|\n|  - college_mathematics                |      1|none  |     4|acc   |\u2191  |0.3200|\u00b1  |0.0469|\n|  - college_physics                    |      1|none  |     4|acc   |\u2191  |0.4412|\u00b1  |0.0494|\n|  - computer_security                  |      1|none  |     4|acc   |\u2191  |0.7500|\u00b1  |0.0435|\n|  - conceptual_physics                 |      1|none  |     4|acc   |\u2191  |0.5745|\u00b1  |0.0323|\n|  - electrical_engineering             |      1|none  |     4|acc   |\u2191  |0.6207|\u00b1  |0.0404|\n|  - elementary_mathematics             |      1|none  |     4|acc   |\u2191  |0.4312|\u00b1  |0.0255|\n|  - high_school_biology                |      1|none  |     4|acc   |\u2191  |0.7452|\u00b1  |0.0248|\n|  - high_school_chemistry              |      1|none  |     4|acc   |\u2191  |0.5172|\u00b1  |0.0352|\n|  - high_school_computer_science       |      1|none  |     4|acc   |\u2191  |0.6700|\u00b1  |0.0473|\n|  - high_school_mathematics            |      1|none  |     4|acc   |\u2191  |0.4037|\u00b1  |0.0299|\n|  - high_school_physics                |      1|none  |     4|acc   |\u2191  |0.4503|\u00b1  |0.0406|\n|  - high_school_statistics             |      1|none  |     4|acc   |\u2191  |0.5046|\u00b1  |0.0341|\n|  - machine_learning                   |      1|none  |     4|acc   |\u2191  |0.5982|\u00b1  |0.0465|\n\n|      Groups      |Version|Filter|n-shot|Metric|   |Value |   |Stderr|\n|------------------|------:|------|------|------|---|-----:|---|-----:|\n|mmlu              |      2|none  |      |acc   |\u2191  |0.6350|\u00b1  |0.0039|\n| - humanities     |      2|none  |      |acc   |\u2191  |0.5881|\u00b1  |0.0068|\n| - other          |      2|none  |      |acc   |\u2191  |0.6962|\u00b1  |0.0080|\n| - social sciences|      2|none  |      |acc   |\u2191  |0.7371|\u00b1  |0.0077|\n| - stem           |      2|none  |      |acc   |\u2191  |0.5449|\u00b1  |0.0086|\n\n"
  },
  "superglue": {
    "boolq": {
      "boolq_acc": 0.8554
    },
    "cb": {
      "cb_acc": 0.8929,
      "cb_f1": 0.8506
    },
    "copa": {
      "copa_acc": 0.93
    },
    "multirc": {
      "multirc_acc": 0.2521
    },
    "record": {
      "record_em": 0.8937,
      "record_f1": 0.9021
    },
    "rte": {
      "rte_acc": 0.8087
    },
    "wic": {
      "wic_acc": 0.5705
    },
    "wsc": {
      "wsc_acc": 0.7596
    }
  },
  "truthfulqa": {
    "truthfulqa_mc1": {
      "truthfulqa_mc1_acc": 0.3195
    },
    "truthfulqa_mc2": {
      "truthfulqa_mc2_acc": 0.5045
    }
  }
}