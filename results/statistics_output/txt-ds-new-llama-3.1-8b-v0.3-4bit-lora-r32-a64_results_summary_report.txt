EVALUATION METRICS SUMMARY REPORT: txt-ds-new-llama-3.1-8b-v0.3-4bit-lora-r32-a64_results.json
================================================================================

Total number of evaluations: 1516

Quality Metrics:
---------------
ANSWER_RELEVANCY     | Mean: 0.9603 | Std: 0.1289 | Range: [0.0000, 1.0000]
FAITHFULNESS         | Mean: 0.8537 | Std: 0.3025 | Range: [0.0000, 1.0000]
CONTEXTUAL_RELEVANCY | Mean: 0.7586 | Std: 0.2616 | Range: [0.0500, 1.0000]
HALLUCINATION        | Mean: 0.4617 | Std: 0.4985 | Range: [0.0000, 1.0000]
FACTUAL_CORRECTNESS  | Mean: 0.6044 | Std: 0.2058 | Range: [0.0928, 1.0000]

BLEU Scores:
-----------
BLEU1                | Mean: 0.1934 | Std: 0.1177 | Range: [0.0000, 0.9130]
BLEU2                | Mean: 0.1017 | Std: 0.0960 | Range: [0.0000, 0.8643]
BLEU3                | Mean: 0.0594 | Std: 0.0834 | Range: [0.0000, 0.8128]
BLEU4                | Mean: 0.0344 | Std: 0.0695 | Range: [0.0000, 0.7522]

ROUGE Scores:
------------
ROUGE1               | Mean: 0.2993 | Std: 0.1168 | Range: [0.0000, 0.9388]
ROUGE2               | Mean: 0.1074 | Std: 0.0935 | Range: [0.0000, 0.8511]
ROUGEL               | Mean: 0.2313 | Std: 0.1032 | Range: [0.0000, 0.9388]

BERT Scores:
-----------
BERT_PRECISION       | Mean: 0.8710 | Std: 0.0325 | Range: [0.5914, 0.9930]
BERT_RECALL          | Mean: 0.8383 | Std: 0.0387 | Range: [0.5049, 0.9930]
BERT_F1              | Mean: 0.8540 | Std: 0.0322 | Range: [0.5801, 0.9930]

KEY INSIGHTS:
-------------
• Highest mean score: answer_relevancy (0.9603)
• Lowest mean score: rouge2 (0.1074)
• Most variable metric: hallucination (std: 0.4985)
• Least variable metric: bert_f1 (std: 0.0322)
