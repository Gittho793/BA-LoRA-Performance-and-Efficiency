"""
Offline LLM Evaluation Pipeline against Ground Truth and expected output using DeepEval

This script reads ground-truth text files and expected answers generated by an LLM,
prompts an LLM for predictions, and computes evaluation metrics using the deepeval library.

Meant to be started from grid_eval.py.
"""
import unsloth  # first for optimization
import os
import time
import json
import argparse
import logging
import gc
import sys
import contextlib
import torch
import numpy as np
from tqdm import tqdm
from unsloth import FastLanguageModel
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from rouge_score import rouge_scorer
from bert_score import score as bert_score
from vllm.distributed import (destroy_distributed_environment,
                              destroy_model_parallel)
from dotenv import load_dotenv

load_dotenv("../../.env")  # necessary for local imports on cluster

project_root = os.path.abspath(
    os.path.join(os.path.dirname(__file__), "../.."))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from src.eval.deepeval_openai import evaluate_with_deepeval


# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def parse_args() -> argparse.Namespace:
    """
    Parse CLI arguments
    """
    parser = argparse.ArgumentParser(
        description="Offline LLM Evaluation"
    )

    # Model arguments
    parser.add_argument('--model-name', required=True,
                        help='Hugging Face model path or repo ID')
    parser.add_argument('--device', default='cpu',
                        help='Device for inference (cpu or cuda)')
    parser.add_argument('--generate', action='store_true',
                        help='Whether to run model.generate()')

    # File paths
    parser.add_argument('--ground-truth', required=True,
                        help='Folder of ground truth .txt files')
    parser.add_argument('--predictions', default='output/predictions',
                        help='Directory to save or load model-specific prediction files')
    parser.add_argument('--questions-json', required=True,
                        help='Path to questions and expected answer json')

    # Generation parameters
    parser.add_argument('--max-new-tokens', type=int, default=4096,
                        help='Max new tokens for generation')
    parser.add_argument('--temperature', type=float, default=0.7,
                        help='Sampling temperature')
    parser.add_argument('--do-sample', action='store_true',
                        help='Use sampling instead of greedy decode')

    # Evaluation metrics
    parser.add_argument('--bleu', action='store_true', help='Compute BLEU')
    parser.add_argument('--bleu-type', choices=["all", 'bleu1', 'bleu2', 'bleu3', 'bleu4'],
                        default='all', help='BLEU type')
    parser.add_argument('--rouge', action='store_true', help='Compute ROUGE')
    parser.add_argument('--bert-score', action='store_true',
                        help='Compute BERTScore')
    parser.add_argument('--bert-model', default='distilbert-base-uncased',  # "google-bert/bert-base-german-cased",
                        help='Model for BERTScore')
    parser.add_argument('--bert-lang', default='de',
                        help='Language for BERTScore')
    parser.add_argument('--deepeval', action='store_true',
                        help='Evaluate using DeepEval metrics')

    return parser.parse_args()


def load_question_json(path: str)-> tuple[dict, dict]:
    """Load questions and answers from JSON file"""
    data = read_json(path)

    question_map, answer_map = {}, {}
    for item in data["questions"]:
        source_file = item["source_file"]
        question_map.setdefault(source_file, []).append(item["question"])
        answer_map.setdefault(source_file, []).append(item["answer"])

    return question_map, answer_map


def build_prompts(question_map: dict) -> dict:
    """
    Build a structured dict mapping from filename to a dict with:
    - intro: prompt intro text 
    - questions: list of numbered question dicts {"number": int, "text": str}
    - outro: prompt outro (e.g. "Antworten:")
    """
    prompts = {}
    for fname, questions in question_map.items():
        q_list = [{"number": i + 1, "text": q}
                  for i, q in enumerate(questions)]
        prompts[fname] = {
            "intro": "Bitte beantworte die folgenden Fragen:",
            "questions": q_list,
            "outro": "Antworten:"
        }

        # print human-readable prompt
        # readable_prompt = f"\n\n{prompts[fname]['intro']}\n" + \
        #                   "\n".join(f"{q['number']}. {q['text']}" for q in q_list) + \
        #                   f"\n\n{prompts[fname]['outro']}"
        # print(readable_prompt)

    return prompts


def read_text_files(folder) -> dict:
    """Read all text files from a folder"""
    texts = {}
    for fname in os.listdir(folder):
        if fname.endswith('.txt'):
            with open(os.path.join(folder, fname), 'r', encoding='utf-8') as f:
                texts[fname] = f.read().strip()
    return texts


def save_predictions(preds: dict, model_name: str, base_path: str = "output/predictions"):
    """Save predictions from the model with model name in filename"""
    just_model_name = os.path.basename(model_name)
    os.makedirs(base_path, exist_ok=True)
    pred_file = os.path.join(base_path, f"{just_model_name}_predictions.json")
    with open(pred_file, 'w', encoding='utf-8') as f:
        json.dump(preds, f, indent=2, ensure_ascii=False)
    return pred_file


def read_json(path: str):
    """Read predictions made by the model"""
    with open(path, 'r', encoding='utf-8') as f:
        return json.load(f)


def cleanup():
    """
    Cleanup GPU Memory.
    Based on https://github.com/vllm-project/vllm/issues/6544
    """
    destroy_model_parallel()
    destroy_distributed_environment()
    with contextlib.suppress(AssertionError):
        torch.distributed.destroy_process_group()
    gc.collect()
    torch.cuda.empty_cache()


def generate_predictions(model, tokenizer, inputs, args) -> dict:
    """Generate predictions using the model with comprehensive GPU memory monitoring"""

    print("\n" + "="*60)
    print("GPU MEMORY MONITORING - GENERATION START")
    print("="*60)

    # Setup variables for monitoring (before generation starts)
    if torch.cuda.is_available():
        gpu_stats = torch.cuda.get_device_properties(0)
        torch.cuda.synchronize()
        torch.cuda.reset_peak_memory_stats()
        start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
        max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
        print(f"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.")
        print(f"{start_gpu_memory} GB of memory reserved.")

    else:
        print("CUDA not available - GPU monitoring disabled")
        gpu_stats, start_gpu_memory, max_memory = None, 0, 0

    # Record start time for generation
    start_time = time.time()

    preds = {}
    total_questions = sum(len(q_list['questions']) for q_list in inputs.values())
    processed_questions = 0

    print(f"Starting generation for {total_questions} questions across {len(inputs)} files")
    print("-" * 60)

    for fname, q_list in tqdm(inputs.items(), desc="Processing files"):
        preds[fname] = []
        for q in tqdm(q_list['questions'], desc=f"Questions in {fname}", leave=False):
            prompt_text = f"Bitte beantworte die folgende Frage:\n{q['number']}. {q['text']}\nAntwort:"

            try:
                encoding = tokenizer(
                    prompt_text,
                    return_tensors="pt",
                    truncation=True,
                    max_length=args.max_new_tokens,
                    padding="longest",
                    return_attention_mask=True,
                )
                input_ids = encoding.input_ids.to(args.device)
                attention_mask = encoding.attention_mask.to(args.device)

                with torch.no_grad():
                    out = model.generate(
                        input_ids,
                        attention_mask=attention_mask,
                        max_new_tokens=args.max_new_tokens,
                        do_sample=args.do_sample,
                        temperature=args.temperature,
                        pad_token_id=tokenizer.eos_token_id,
                        use_cache=True
                    )

                new_tokens = out[0][len(input_ids[0]):]
                pred_text = tokenizer.decode(new_tokens, skip_special_tokens=True)

            except Exception as e:  # unknown Exception type
                logger.error(f"Error generating prediction for question in {fname}: {str(e)}")
                pred_text = ""

            # Append dictionary with question, predicted answer
            preds[fname].append({
                "question": q['text'],  # just question text, without numbers/prefix
                "predicted_answer": pred_text,
            })

            processed_questions += 1

    # Record end time and calculate final statistics
    end_time = time.time()
    generation_runtime = end_time - start_time

    print("\n" + "="*60)
    print("GPU MEMORY MONITORING - GENERATION COMPLETE")
    print("="*60)

    if torch.cuda.is_available():
        # Get final stats after generation
        used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
        used_memory_for_generation = round(used_memory - start_gpu_memory, 3)
        used_percentage = round(used_memory / max_memory * 100, 3)
        generation_percentage = round(used_memory_for_generation / max_memory * 100, 3)

        # Print generation statistics
        print(f"{generation_runtime:.2f} seconds used for generation.")
        print(f"{round(generation_runtime/60, 2)} minutes used for generation.")
        print(f"Peak reserved memory = {used_memory} GB.")
        print(f"Peak reserved memory for generation = {used_memory_for_generation} GB.")
        print(f"Peak reserved memory % of max memory = {used_percentage} %.")
        print(f"Peak reserved memory for generation % of max memory = {generation_percentage} %.")

        # Create output directory and save GPU usage statistics
        os.makedirs(os.path.dirname(args.predictions), exist_ok=True)

        # Write to file
        stats_file = os.path.join(os.path.dirname(args.predictions), f"{args.model_name}-generation_gpu_usage.txt")
        with open(stats_file, "w", encoding="utf-8") as f:
            f.write(f"""Peak reserved memory = {used_memory} GB.
Peak reserved memory for generation = {used_memory_for_generation} GB.
Peak reserved memory % of max memory = {used_percentage} %.
Peak reserved memory for generation % of max memory = {generation_percentage} %.
{round(generation_runtime/60, 2)} minutes used for generation.

Additional Statistics:
GPU Device: {gpu_stats.name}
Total GPU Memory: {max_memory} GB
Starting Memory Reserved: {start_gpu_memory} GB
Total Questions Processed: {processed_questions}
Generation Runtime: {generation_runtime:.2f} seconds
Average Time per Question: {round(generation_runtime/processed_questions, 2)} seconds

Model Configuration:
Max New Tokens: {args.max_new_tokens}
Temperature: {args.temperature}
Sampling: {args.do_sample}
Device: {args.device}
""")

        print(f"GPU usage statistics saved to: {stats_file}")
    else:
        print(f"Generation completed in {round(generation_runtime/60, 2)} minutes")
        print("CUDA not available - no GPU statistics collected")

    return preds


def evaluate_optimized_deepeval(deepeval_data, args):
    """
    Optimized evaluate function with memory management for BERT score
    Modified to work with deepeval JSON structure:
    {
      "filename.txt": [
        {
          "question": "...",
          "expected": "...", 
          "predicted": "...",
          "metrics": {...}
        },
        ...
      ]
    }

    Args:
        deepeval_data: Dictionary with the structure shown above
        args: Argument object with evaluation flags (bleu, rouge, bert_score, etc.)

    Returns:
        Dictionary with same structure but additional computed metrics per question
    """

    # Enable mixed precision if available
    use_amp = torch.cuda.is_available() and hasattr(torch.cuda.amp, 'autocast')

    # Set memory fraction to prevent over-allocation
    if torch.cuda.is_available():
        torch.cuda.set_per_process_memory_fraction(
            0.8)  # Use 80% of GPU memory max

    rouge_inst = rouge_scorer.RougeScorer(
        ['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)

    bleu_weights = {
        'bleu1': (1, 0, 0, 0),
        'bleu2': (0.5, 0.5, 0, 0),
        'bleu3': (0.33, 0.33, 0.33, 0),
        'bleu4': (0.25, 0.25, 0.25, 0.25)
    }

    results = {}

    # Process in smaller chunks to manage memory
    chunk_size = 2  # Process 2 files at a time
    file_items = list(deepeval_data.items())

    for chunk_start in tqdm(range(0, len(file_items), chunk_size), desc="Evaluating simple metrics."):
        chunk_end = min(chunk_start + chunk_size, len(file_items))
        chunk_items = file_items[chunk_start:chunk_end]

        # Clear GPU cache before processing each chunk
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        for fname, questions_list in chunk_items:
            # Results for this file
            file_results = []

            for i, qa_item in enumerate(questions_list):
                question = qa_item.get('question', '')
                expected = qa_item.get('expected', '')
                predicted = qa_item.get('predicted', '')
                existing_metrics = qa_item.get('metrics', {})

                res = {
                    'question': question,
                    'expected': expected,
                    'predicted': predicted,
                    'existing_metrics': existing_metrics
                }

                # Only compute new metrics if we have both expected and predicted text
                if expected and predicted:
                    # BLEU score calculation (no memory issues)
                    if hasattr(args, 'bleu') and args.bleu:
                        if hasattr(args, 'bleu_type') and args.bleu_type == 'all':
                            for btype, weights in bleu_weights.items():
                                try:
                                    res[btype] = sentence_bleu(
                                        [expected.split()], predicted.split(), weights=weights)
                                except Exception as e:
                                    print(
                                        f"Error computing {btype} for {fname} Q{i+1}: {e}")
                                    res[btype] = 0.0
                        else:
                            bleu_type = getattr(args, 'bleu_type', 'bleu4')
                            weights = bleu_weights.get(
                                bleu_type, bleu_weights['bleu4'])
                            try:
                                smoothie = SmoothingFunction().method1
                                res[bleu_type] = sentence_bleu(
                                    [expected.split()], predicted.split(), weights=weights, smoothing_function=smoothie)
                            except Exception as e:
                                print(
                                    f"Error computing {bleu_type} for {fname} Q{i+1}: {e}")
                                res[bleu_type] = 0.0

                    # ROUGE score calculation (no memory issues)
                    if hasattr(args, 'rouge') and args.rouge:
                        try:
                            sc = rouge_inst.score(expected, predicted)
                            res['rouge1'] = sc['rouge1'].fmeasure
                            res['rouge2'] = sc['rouge2'].fmeasure
                            res['rougeL'] = sc['rougeL'].fmeasure
                        except Exception as e:
                            print(
                                f"Error computing ROUGE for {fname} Q{i+1}: {e}")
                            res['rouge1'] = res['rouge2'] = res['rougeL'] = 0.0

                    # BERT score calculation with memory optimizations
                    if hasattr(args, 'bert_score') and args.bert_score:
                        try:
                            with torch.amp.autocast("cuda", enabled=False):  # disable AMP for BERT score at first for stability

                                precision, recall, f1_score = bert_score(
                                    [predicted], [expected],
                                    model_type=getattr(
                                        args, 'bert_model', 'distilbert-base-uncased'),
                                    lang=getattr(args, 'bert_lang', 'de'),
                                    batch_size=1,  # Process one at a time for memory
                                    rescale_with_baseline=False,
                                    device='cuda' if torch.cuda.is_available() else 'cpu'  # use GPU if available
                                )

                            res['bert_precision'] = precision[0].item()
                            res['bert_recall'] = recall[0].item()
                            res['bert_f1'] = f1_score[0].item()

                        except RuntimeError as e:
                            if "out of memory" in str(e):
                                print(
                                    f"GPU OOM for {fname} Q{i+1}, falling back to smaller model...")
                                # Fallback to CPU processing
                                torch.cuda.empty_cache()
                                try:
                                    precision, recall, f1_score = bert_score(
                                        [predicted], [expected],
                                        model_type='distilbert-base-uncased',
                                        lang=getattr(args, 'bert_lang', 'de'),
                                        batch_size=1,
                                        rescale_with_baseline=False,
                                        device='cpu'
                                    )
                                    res['bert_precision'] = precision[0].item()
                                    res['bert_recall'] = recall[0].item()
                                    res['bert_f1'] = f1_score[0].item()
                                except Exception as inner_e:
                                    print(
                                        f"Error computing BERT score for {fname} Q{i+1}: {inner_e}")
                                    res['bert_precision'] = res['bert_recall'] = res['bert_f1'] = 0.0
                            else:
                                print(
                                    f"Error computing BERT score for {fname} Q{i+1}: {e}")
                                res['bert_precision'] = res['bert_recall'] = res['bert_f1'] = 0.0
                        except Exception as e:
                            print(
                                f"Error computing BERT score for {fname} Q{i+1}: {e}")
                            res['bert_precision'] = res['bert_recall'] = res['bert_f1'] = 0.0

                file_results.append(res)

                # Clean up GPU memory after each question
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()

            results[fname] = file_results

        # Clean up chunk variables
        del chunk_items
        gc.collect()

    return results


def aggregate_question_metrics(results) -> dict[str, dict]:
    """
    Aggregate metrics across all questions to get file-level and overall statistics

    Args:
        results: Output from evaluate_optimized_deepeval

    Returns:
        Dictionary with aggregated metrics
    """

    aggregated = {
        'per_file': {},
        'overall': {}
    }

    all_metrics = {}

    for fname, questions in results.items():
        file_metrics = {}

        for qa_result in questions:
            # Collect all numeric metrics (excluding text fields)
            for key, value in qa_result.items():
                if key not in ['question', 'expected', 'predicted', 'existing_metrics'] and isinstance(value, (int, float)):
                    if key not in file_metrics:
                        file_metrics[key] = []
                    file_metrics[key].append(value)

                    # Also collect for overall stats
                    if key not in all_metrics:
                        all_metrics[key] = []
                    all_metrics[key].append(value)

        # Calculate file-level averages
        file_averages = {}
        for metric, values in file_metrics.items():
            if values:
                file_averages[f'{metric}_mean'] = np.mean(values)
                file_averages[f'{metric}_std'] = np.std(values)
                file_averages[f'{metric}_count'] = len(values)

        aggregated['per_file'][fname] = file_averages

    # Calculate overall averages
    overall_averages = {}
    for metric, values in all_metrics.items():
        if values:
            overall_averages[f'{metric}_mean'] = np.mean(values)
            overall_averages[f'{metric}_std'] = np.std(values)
            overall_averages[f'{metric}_count'] = len(values)

    aggregated['overall'] = overall_averages

    return aggregated


def save_evaluation_results(results, aggregated, output_file) -> None:
    """Save evaluation results to JSON file"""

    output_data = {
        'detailed_results': results,
        'aggregated_metrics': aggregated
    }

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, indent=2, ensure_ascii=False)

    print(f"Results saved to {output_file}")


def main():
    """
    Main function to run the evaluation
    """
    try:
        args = parse_args()
        logger.info(f"Arguments: {args}")

        # Read ground truth texts
        gt_texts = read_text_files(args.ground_truth)
        logger.info(f"Loaded {len(gt_texts)} ground truth files")

        # Load questions and answers from JSON
        question_map, answer_map = load_question_json(args.questions_json)
        logger.info(f"Loaded questions for {len(question_map)} files")

        if args.generate:
            logger.info(f"Loading model: {args.model_name}")

            model, tokenizer = FastLanguageModel.from_pretrained(
                model_name=args.model_name,
                load_in_4bit=False
            )

            FastLanguageModel.for_inference(model)

            model = torch.compile(model)

            if tokenizer.pad_token is None or tokenizer.pad_token_id == tokenizer.eos_token_id:
                tokenizer.add_special_tokens({'pad_token': '[PAD]'})
                # Update model embeddings because added a new token
                model.resize_token_embeddings(len(tokenizer))

            prompts = build_prompts(question_map)

            # Generate predictions
            preds = generate_predictions(model, tokenizer, prompts, args)
            pred_file = save_predictions(
                preds, args.model_name, args.predictions)
            torch.cuda.empty_cache()
        else:
            # Load existing predictions
            just_model_name = os.path.basename(args.model_name)
            pred_file = os.path.join(
                args.predictions, f"{just_model_name}_predictions.json")
            preds = read_json(pred_file)

        if args.deepeval:
            logger.info("Running DeepEval evaluation...")

            deepeval_results = evaluate_with_deepeval(
                gt_texts, preds, question_map, answer_map)

            print("\n=== DeepEval Results ===")
            for fname, results in deepeval_results.items():
                print(f"\nFile: {fname}")
                for idx, result in enumerate(results):
                    print(f"  Q{idx + 1}: {result['question']}")
                    for metric, details in result['metrics'].items():
                        print(
                            f"    {metric}: {details['score']:.4f}, Success: {details['success']}, Reason: {details['reason']}")

            just_model_name = os.path.basename(args.model_name)

            with open(f"../../results/deepeval/deepeval_{just_model_name}_results.json", 'w', encoding="utf-8") as f:
                json.dump(deepeval_results, f, indent=2)

        # Evaluate predictions
        if any([args.bleu, args.rouge, args.bert_score]):
            just_model_name = os.path.basename(args.model_name)
            deepeval_data = read_json(
                f"../../results/deepeval/deepeval_{just_model_name}_results.json")

            logger.info("Evaluating predictions...")
            results = evaluate_optimized_deepeval(deepeval_data, args)
            aggregated = aggregate_question_metrics(results)

            print("\n=== Evaluation Summary ===")
            for metric, value in aggregated['overall'].items():
                if metric.endswith('_mean'):
                    metric_name = metric.replace('_mean', '')
                    std_key = metric.replace('_mean', '_std')
                    count_key = metric.replace('_mean', '_count')
                    std_value = aggregated['overall'].get(std_key, 0)
                    count_value = aggregated['overall'].get(count_key, 0)
                    print(
                        f"{metric_name}: {value:.4f} Â± {std_value:.4f} (n={count_value})")

            just_model_name = os.path.basename(args.model_name)
            save_evaluation_results(
                results, aggregated, f"../../results/metrics_deepeval/{just_model_name}_results.json")

    except Exception as e:
        print(f"Error during evaluation: {e}")
        raise
    finally:
        torch.cuda.empty_cache()
        print("GPU cache cleared.")
        sys.exit()


if __name__ == '__main__':
    main()
